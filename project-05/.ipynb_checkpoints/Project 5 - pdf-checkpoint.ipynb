{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandoc\n",
      "  Downloading pandoc-1.0.0b2.tar.gz (516kB)\n",
      "Requirement already satisfied: ply in c:\\users\\ali\\anaconda2\\lib\\site-packages (from pandoc)\n",
      "Building wheels for collected packages: pandoc\n",
      "  Running setup.py bdist_wheel for pandoc: started\n",
      "  Running setup.py bdist_wheel for pandoc: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\ali\\AppData\\Local\\pip\\Cache\\wheels\\6f\\50\\45\\4477787679133c1ccdb0f65c324cb05fd40e679767b89db6b0\n",
      "Successfully built pandoc\n",
      "Installing collected packages: pandoc\n",
      "Successfully installed pandoc-1.0.0b2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we imployed feature selection and classificaion techniques to analyze and model a synthic dataset. The goal is to have the highest accuracy score, while ammplifing the effect of salient features and supressing the effect of noise and less important features.\n",
    "\n",
    "For this purpose we have defined four wrappers to help create a pipeline to load the data from a sql database, create a data dictionary, which will be inputed to a transformer and finally a general model which will use the data dictionary output of the transformer to score the model. These wrapper function - located at the lib folder - are:\n",
    "\n",
    "    - load_data_from_dtabase: connect and download data from a sql database and load the data into a dataframe.\n",
    "    - make_data_dict: splits the dataframe into test and train sets, test size and random state can be modified and returns a data dictionary.\n",
    "    - general_transformer: which uses the data dicrionary to apply a transformer which can be modified, and (scales and) transforms the data.\n",
    "    - general_model: finally the model is inputed through this function and we will have the test/train score and hence the accuracy score.\n",
    "\n",
    "The data set is called MADELON, an artificial and highly non linear dataset with 2000 instances and and 500 features. Do to the stochastic nature of the dataset we would like to reduce the number of features while obtaining the best accuracy score possible.\n",
    "\n",
    "We start with a logistic regression with a default penatly of l2. The accuracy score is 0.53 and all the feautres have an effect on the model and the score. Changing the penalty to l1 actually had a small negative effect on the score but it reduce the number of features to 453. Still high for this dataset.\n",
    "\n",
    "Next step, we experiment with different inverse of gama values or C, from 0.001 to 1000 through a grid search along with changing the penalty between l1 and l2. The final result shows significant feature reduction - to two features, 475 and 241 - however the accuracy score is, despite an increase, is still low, at 0.62.\n",
    "\n",
    "So at the third and final step we wil deploy grid search to apply it to our models. In addition to our logistic classifie we will use the kNN classifier. We will scale the data for both models, and we will use k Best transformer and input the data to logistic regression and kNN classifiers.\n",
    "\n",
    "To see the best result of the combination of kBest/LogisticRegression and kBest/kNN models we will use grid search to searfch and find the best parameters for our transfomer and classifiers. \n",
    "\n",
    "The k range is set to 3 to 30. Hopefully the best k is in this range. We change the penalty between l1 and l2 and C from 0.001, 1 and 100. The best result we get is with l2 penalty, a C of 0.0212 and a test score of 0.64. The best model is:\n",
    "    LogisticRegression(C=0.021181818181818184, class_weight=None, dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
    "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))\n",
    "We would like to have a better score. So we try kNN.\n",
    "\n",
    "At this stage we kNN model along with same k Best transformer. The best result is achieved with k=13, n-neighbors=5 and the test score is 0.88.\n",
    "\n",
    "So to conclude we achieved our goal, best accuracy score, by utilizing a standard scaler, a kBest transformer and kNN classifier and we improved the score from 0.53 with a default logistic regression to 0.88."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
