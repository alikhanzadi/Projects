{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib.project_5 import load_data_from_database, make_data_dict, general_model, general_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Identify Salient Features Using $\\ell1$-penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: EACH OF THESE SHOULD BE WRITTEN SOLELY WITH REGARD TO STEP 2 - Identify Features**\n",
    "\n",
    "### Domain and Data\n",
    "\n",
    "**TODO:** Write a simple statement about the domain of your problem and the dataset upon which you will be working. \n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "**TODO:** At this part we want to scale and transorm the model so that we have feature reduction, in order to distinguish salient features from noise.\n",
    "\n",
    "### Solution Statement\n",
    "\n",
    "**TODO:** We apply the l1 penatly in the model to reduce the coeficient of insignificant features. We want to find the salient features.\n",
    "\n",
    "### Metric\n",
    "\n",
    "**TODO**: Again we will use the accuracy score as the main indication if the model is performing will, but in addition, we want to monitor the number of features and optimally reduce the number of features.\n",
    "\n",
    "### Benchmark\n",
    "\n",
    "**TODO**: Introducing the l1 penalty actually harmed the accuracy scored, although not too much. However it did have a positive effect on the number of features and reduced it to 453.\n",
    "\n",
    "By adding a k Best transformer and appling a grid search, we were able to reduce the number of features to two. The final accuracy score is 0.62 and the AUC is 0.593. Despite the feature reduction the model still needs to be improved.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Implement the following code pipeline using the functions you write in `lib/project_5.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/identify_features.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "madelon_df = load_data_from_database('joshuacook.me', '5432',\n",
    "                           'dsi', 'madelon', 'dsi_student',\n",
    "                           'correct horse battery staple')\n",
    "madelon_df = madelon_df.drop('index', axis = 1)\n",
    "X = madelon_df.drop('label', axis = 1)\n",
    "y = madelon_df.label\n",
    "split_data_dict = make_data_dict(X, y, random_state=42)\n",
    "final_data = general_transformer(StandardScaler(), split_data_dict)\n",
    "logisticRegression_l1_C1 = general_model(LogisticRegression(penalty='l1', C=1), final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " 'test_score': 0.52833333333333332,\n",
       " 'train_score': 0.8035714285714286}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegression_l1_C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_ = final_data['X_train']\n",
    "y_ = final_data['y_train']\n",
    "X__ = final_data['X_test']\n",
    "y__ = final_data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Also manauly we can check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8035714285714286, 0.52833333333333332)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l1').fit(X_,y_)\n",
    "tr = lr.score(X_, y_)\n",
    "ts = lr.score(X__,y__)\n",
    "print (tr,ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top features for Log. Regr. with penalty l1 and c=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat.</th>\n",
       "      <th>coef.</th>\n",
       "      <th>abscoef.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>feat_442</td>\n",
       "      <td>-0.793853</td>\n",
       "      <td>0.793853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>feat_153</td>\n",
       "      <td>-0.653517</td>\n",
       "      <td>0.653517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>feat_241</td>\n",
       "      <td>0.626621</td>\n",
       "      <td>0.626621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>feat_472</td>\n",
       "      <td>0.430815</td>\n",
       "      <td>0.430815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>feat_424</td>\n",
       "      <td>0.319375</td>\n",
       "      <td>0.319375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>feat_046</td>\n",
       "      <td>0.302231</td>\n",
       "      <td>0.302231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>feat_056</td>\n",
       "      <td>0.298208</td>\n",
       "      <td>0.298208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>feat_048</td>\n",
       "      <td>0.295484</td>\n",
       "      <td>0.295484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>feat_493</td>\n",
       "      <td>0.285366</td>\n",
       "      <td>0.285366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>feat_494</td>\n",
       "      <td>0.276396</td>\n",
       "      <td>0.276396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>feat_496</td>\n",
       "      <td>-0.272265</td>\n",
       "      <td>0.272265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>feat_425</td>\n",
       "      <td>-0.264756</td>\n",
       "      <td>0.264756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>feat_161</td>\n",
       "      <td>-0.260254</td>\n",
       "      <td>0.260254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>feat_130</td>\n",
       "      <td>-0.250915</td>\n",
       "      <td>0.250915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>feat_423</td>\n",
       "      <td>-0.249310</td>\n",
       "      <td>0.249310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feat.     coef.  abscoef.\n",
       "442  feat_442 -0.793853  0.793853\n",
       "153  feat_153 -0.653517  0.653517\n",
       "241  feat_241  0.626621  0.626621\n",
       "472  feat_472  0.430815  0.430815\n",
       "424  feat_424  0.319375  0.319375\n",
       "46   feat_046  0.302231  0.302231\n",
       "56   feat_056  0.298208  0.298208\n",
       "48   feat_048  0.295484  0.295484\n",
       "493  feat_493  0.285366  0.285366\n",
       "494  feat_494  0.276396  0.276396\n",
       "496  feat_496 -0.272265  0.272265\n",
       "425  feat_425 -0.264756  0.264756\n",
       "161  feat_161 -0.260254  0.260254\n",
       "130  feat_130 -0.250915  0.250915\n",
       "423  feat_423 -0.249310  0.249310"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_co = []\n",
    "for i,j in enumerate (X.columns):\n",
    "    feature_co.append([j, (logisticRegression_l1_C1['model'].coef_[0][i]),\n",
    "                       abs(logisticRegression_l1_C1['model'].coef_[0][i])])\n",
    "    \n",
    "log_reg_coef_l1_C1 = pd.DataFrame(feature_co,columns=['feat.','coef.','abscoef.'])\n",
    "log_reg_coef_l1_C1.sort_values(['abscoef.'],ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "453"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([coef for coef in logisticRegression_l1_C1['model'].coef_[0] if abs(coef) > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch to find if l1 or l2 gives better result with what values of C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': GridSearchCV(cv=None, error_score='raise',\n",
       "        estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       "        fit_params={}, iid=True, n_jobs=1,\n",
       "        param_grid={'penalty': ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
       "        pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "        scoring=None, verbose=0),\n",
       " 'test_score': 0.59333333333333338,\n",
       " 'train_score': 0.62285714285714289}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_lr_params = {\n",
    "    'penalty' : [\"l1\", \"l2\"],\n",
    "    'C' : [0.001,0.01,0.1,1,10,100,1000]    \n",
    "}\n",
    "\n",
    "gridsearch_result = GridSearchCV(LogisticRegression(), \n",
    "                                 param_grid=gridsearch_lr_params)\n",
    "gridsearch_lr = general_model(gridsearch_result,final_data)\n",
    "gridsearch_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_lr[\"model\"].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62214285714285711"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_lr[\"model\"].best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_lr[\"model\"].best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031667</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.503571</td>\n",
       "      <td>0.503572</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>{u'penalty': u'l1', u'C': 0.001}</td>\n",
       "      <td>14</td>\n",
       "      <td>0.503212</td>\n",
       "      <td>0.503751</td>\n",
       "      <td>0.503212</td>\n",
       "      <td>0.503751</td>\n",
       "      <td>0.504292</td>\n",
       "      <td>0.503212</td>\n",
       "      <td>0.005735</td>\n",
       "      <td>4.713704e-04</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.602143</td>\n",
       "      <td>0.773927</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l2</td>\n",
       "      <td>{u'penalty': u'l2', u'C': 0.001}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.586724</td>\n",
       "      <td>0.789925</td>\n",
       "      <td>0.631692</td>\n",
       "      <td>0.753483</td>\n",
       "      <td>0.587983</td>\n",
       "      <td>0.778373</td>\n",
       "      <td>0.001633</td>\n",
       "      <td>1.123916e-07</td>\n",
       "      <td>0.020912</td>\n",
       "      <td>0.015206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.028667</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.622143</td>\n",
       "      <td>0.623928</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>{u'penalty': u'l1', u'C': 0.01}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.631692</td>\n",
       "      <td>0.619507</td>\n",
       "      <td>0.616702</td>\n",
       "      <td>0.627010</td>\n",
       "      <td>0.618026</td>\n",
       "      <td>0.625268</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>1.123916e-07</td>\n",
       "      <td>0.006777</td>\n",
       "      <td>0.003206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.089000</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.569286</td>\n",
       "      <td>0.849283</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>{u'penalty': u'l2', u'C': 0.01}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.565310</td>\n",
       "      <td>0.859593</td>\n",
       "      <td>0.580300</td>\n",
       "      <td>0.830654</td>\n",
       "      <td>0.562232</td>\n",
       "      <td>0.857602</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>4.714827e-04</td>\n",
       "      <td>0.007893</td>\n",
       "      <td>0.013198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.127000</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.577857</td>\n",
       "      <td>0.806420</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>{u'penalty': u'l1', u'C': 0.1}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.571734</td>\n",
       "      <td>0.801715</td>\n",
       "      <td>0.597430</td>\n",
       "      <td>0.786710</td>\n",
       "      <td>0.564378</td>\n",
       "      <td>0.830835</td>\n",
       "      <td>0.041045</td>\n",
       "      <td>4.714266e-04</td>\n",
       "      <td>0.014170</td>\n",
       "      <td>0.018319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_C  \\\n",
       "0       0.031667         0.001333         0.503571          0.503572   0.001   \n",
       "1       0.048000         0.001000         0.602143          0.773927   0.001   \n",
       "2       0.028667         0.001000         0.622143          0.623928    0.01   \n",
       "3       0.089000         0.001333         0.569286          0.849283    0.01   \n",
       "4       0.127000         0.001333         0.577857          0.806420     0.1   \n",
       "\n",
       "  param_penalty                            params  rank_test_score  \\\n",
       "0            l1  {u'penalty': u'l1', u'C': 0.001}               14   \n",
       "1            l2  {u'penalty': u'l2', u'C': 0.001}                2   \n",
       "2            l1   {u'penalty': u'l1', u'C': 0.01}                1   \n",
       "3            l2   {u'penalty': u'l2', u'C': 0.01}                4   \n",
       "4            l1    {u'penalty': u'l1', u'C': 0.1}                3   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0           0.503212            0.503751           0.503212   \n",
       "1           0.586724            0.789925           0.631692   \n",
       "2           0.631692            0.619507           0.616702   \n",
       "3           0.565310            0.859593           0.580300   \n",
       "4           0.571734            0.801715           0.597430   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0            0.503751           0.504292            0.503212      0.005735   \n",
       "1            0.753483           0.587983            0.778373      0.001633   \n",
       "2            0.627010           0.618026            0.625268      0.000943   \n",
       "3            0.830654           0.562232            0.857602      0.004967   \n",
       "4            0.786710           0.564378            0.830835      0.041045   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0    4.713704e-04        0.000509         0.000254  \n",
       "1    1.123916e-07        0.020912         0.015206  \n",
       "2    1.123916e-07        0.006777         0.003206  \n",
       "3    4.714827e-04        0.007893         0.013198  \n",
       "4    4.714266e-04        0.014170         0.018319  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_lr_df = pd.DataFrame(gridsearch_lr['model'].cv_results_)\n",
    "gridsearch_lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top features for Log. Regr. for best estimator (Penalty = l1 and C=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat.</th>\n",
       "      <th>coef.</th>\n",
       "      <th>abscoef.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>feat_475</td>\n",
       "      <td>0.102169</td>\n",
       "      <td>0.102169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>feat_241</td>\n",
       "      <td>0.076752</td>\n",
       "      <td>0.076752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feat_000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>feat_329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>feat_341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>feat_340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>feat_339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>feat_338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>feat_337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>feat_336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>feat_335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>feat_334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>feat_333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>feat_332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>feat_331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feat.     coef.  abscoef.\n",
       "475  feat_475  0.102169  0.102169\n",
       "241  feat_241  0.076752  0.076752\n",
       "0    feat_000  0.000000  0.000000\n",
       "329  feat_329  0.000000  0.000000\n",
       "341  feat_341  0.000000  0.000000\n",
       "340  feat_340  0.000000  0.000000\n",
       "339  feat_339  0.000000  0.000000\n",
       "338  feat_338  0.000000  0.000000\n",
       "337  feat_337  0.000000  0.000000\n",
       "336  feat_336  0.000000  0.000000\n",
       "335  feat_335  0.000000  0.000000\n",
       "334  feat_334  0.000000  0.000000\n",
       "333  feat_333  0.000000  0.000000\n",
       "332  feat_332  0.000000  0.000000\n",
       "331  feat_331  0.000000  0.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_co = []\n",
    "for i,j in enumerate (X.columns):\n",
    "    feature_co.append([j, (logisticRegressionn_l1_C01['model'].coef_[0][i]),\n",
    "                       abs(logisticRegressionn_l1_C01['model'].coef_[0][i])])\n",
    "    \n",
    "log_reg_coef_l1_C01 = pd.DataFrame(feature_co,columns=['feat.','coef.','abscoef.'])\n",
    "log_reg_coef_l1_C01.sort_values(['abscoef.'],ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([coef for coef in logisticRegressionn_l1_C01['model'].coef_[0] if abs(coef) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = logisticRegressionn_l1_C01['model'].predict(X__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[174, 121],\n",
       "       [123, 182]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_mtx = confusion_matrix(y__,y_pred)\n",
    "confusion_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59327590997499313"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_1_error, true_positive, _ = roc_curve(y__,y_pred)\n",
    "area_under_curve_lr = auc(type_1_error, true_positive)\n",
    "area_under_curve_lr"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
